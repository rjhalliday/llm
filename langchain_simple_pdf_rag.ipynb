{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjhalliday/python-llm/blob/main/langchain_simple_pdf_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple langchain rag. We load all documents in the pdf folder, however in this specific example I've only placed a single PDF in the folder as otherwise loading the PDFs takes a very long time due to the free Fremini rate limit."
      ],
      "metadata": {
        "id": "6yb_c6OVhy2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuIv0IQ6zMQa",
        "outputId": "f6d99792-76e7-4316-daab-c84eb5c44b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m61.4/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# rh\n",
        "!pip install -qU langchain-google-genai langchain-community chromadb pymupdf pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5NBX_Z_fypVz"
      },
      "outputs": [],
      "source": [
        "# Langchain dependencies\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
        "from langchain.embeddings import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
        "from langchain.schema import Document # Importing Document schema from Langchain\n",
        "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
        "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
        "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
        "import os # Importing os module for operating system functionalities\n",
        "import shutil # Importing shutil module for high-level file operations\n",
        "\n",
        "\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "import textwrap\n",
        "from pprint import pprint\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b5Q3Iwuy9lxn"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize the language model using GoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = GooglePalmEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDpq8FryYrp",
        "outputId": "534cc110-d939-4151-dcaf-6b370e10f37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content=' \n",
            " Data -Over -Cable Service Interface Specifications \n",
            "DOCSIS 2.0 + IPv6 Cable Modem Specification  \n",
            "CM-SP-DOCSIS2.0 -IPv6 -I07-130404  \n",
            "ISSUED  \n",
            "Notice  \n",
            "This DOCSIS ® specification is the result of a cooperative effort \n",
            "undertaken at the direction of Cable Television Laboratories, Inc. for the \n",
            "benefit of the cable industry and its customers. This document may \n",
            "contain references to other documents not owned or controlled by CableLabs ®. Use and understanding of this document may require \n",
            "access to such other documents. Designing, manufacturing, distributing, \n",
            "using, selling, or servicing products, or providing services, based on this \n",
            "document may require intellectual property licenses from third parties for technology referenced in this document.  \n",
            "Neither CableLabs nor any member company is responsible to any \n",
            "party for any liability of any nature whatsoever resulting from or arising \n",
            "out of use or reliance upon this document, or any document referenced \n",
            "herein. This document is furnished on an \"AS IS\" basis and neither \n",
            "CableLabs nor its members provides any representation or warranty, \n",
            "express or implied, regarding the accuracy, completeness, \n",
            "noninfringement, or fitness for a particular purpose of this document, o r \n",
            "any document referenced herein.  \n",
            " Cable Television Laboratories, Inc. , 2009-2013  \n",
            " \n",
            "  ' metadata={'source': '/content/drive/MyDrive/pdfs/CM-SP-DOCSIS2.0-IPv6-I07-130404.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "# Directory to your pdf files:\n",
        "DATA_PATH = \"/content/drive/MyDrive/pdfs\"\n",
        "def load_documents():\n",
        "  \"\"\"\n",
        "  Load PDF documents from the specified directory using PyPDFDirectoryLoader.\n",
        "  Returns:\n",
        "  List of Document objects: Loaded PDF documents represented as Langchain\n",
        "                                                          Document objects.\n",
        "  \"\"\"\n",
        "  # Initialize PDF loader with specified directory\n",
        "  document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "  # Load PDF documents and return them as a list of Document objects\n",
        "  return document_loader.load()\n",
        "\n",
        "documents = load_documents() # Call the function\n",
        "# Inspect the contents of the first document as well as metadata\n",
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JiSIp2a7xZG"
      },
      "source": [
        "# chuncking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3-pTy8o9yr3L"
      },
      "outputs": [],
      "source": [
        "def split_text(documents: list[Document]):\n",
        "  \"\"\"\n",
        "  Split the text content of the given list of Document objects into smaller chunks.\n",
        "  Args:\n",
        "    documents (list[Document]): List of Document objects containing text content to split.\n",
        "  Returns:\n",
        "    list[Document]: List of Document objects representing the split text chunks.\n",
        "  \"\"\"\n",
        "  # Initialize text splitter with specified parameters\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300, # Size of each chunk in characters\n",
        "    chunk_overlap=100, # Overlap between consecutive chunks\n",
        "    length_function=len, # Function to compute the length of the text\n",
        "    add_start_index=True, # Flag to add start index to each chunk\n",
        "  )\n",
        "\n",
        "  # Split documents into smaller chunks using text splitter\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "  # Print example of page content and metadata for a chunk\n",
        "  document = chunks[0]\n",
        "  print(document.page_content)\n",
        "  print(document.metadata)\n",
        "\n",
        "  return chunks # Return the list of split text chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IDc82xrpy676"
      },
      "outputs": [],
      "source": [
        "# Path to the directory to save Chroma database - save it on google drive\n",
        "#CHROMA_PATH = \"/content/drive/MyDrive/chroma\"\n",
        "CHROMA_PATH = \"chroma\"\n",
        "COLLECTION_NAME=\"text-collection\"\n",
        "\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    \"\"\"\n",
        "    Save the given list of Document objects to a Chroma database, updating existing data if present.\n",
        "    Args:\n",
        "    chunks (list[Document]): List of Document objects representing text chunks to save.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Check if the Chroma vector store exists\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        # Load the existing Chroma database\n",
        "        client = Chroma.load(persist_directory=CHROMA_PATH, collection_name=COLLECTION_NAME)\n",
        "    else:\n",
        "        # Create a new Chroma database if it doesn't exist\n",
        "        client = Chroma.from_documents(\n",
        "            [],\n",
        "            embeddings,\n",
        "            persist_directory=CHROMA_PATH,\n",
        "            collection_name=COLLECTION_NAME\n",
        "        )\n",
        "\n",
        "    # Add new documents to the existing or new Chroma database\n",
        "    client.add_documents(chunks)\n",
        "\n",
        "    # Persist the updated database to disk\n",
        "    client.persist()\n",
        "    print(f\"Updated Chroma database with {len(chunks)} new chunks at {CHROMA_PATH}.\")\n",
        "    return client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "kZxkrFhSy9Tj",
        "outputId": "1b69d1de-2498-4357-b65a-bd67c71497bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 26 documents into 280 chunks.\n",
            "Data -Over -Cable Service Interface Specifications \n",
            "DOCSIS 2.0 + IPv6 Cable Modem Specification  \n",
            "CM-SP-DOCSIS2.0 -IPv6 -I07-130404  \n",
            "ISSUED  \n",
            "Notice  \n",
            "This DOCSIS ® specification is the result of a cooperative effort \n",
            "undertaken at the direction of Cable Television Laboratories, Inc. for the\n",
            "{'source': '/content/drive/MyDrive/pdfs/CM-SP-DOCSIS2.0-IPv6-I07-130404.pdf', 'page': 0, 'start_index': 3}\n",
            "Saved 280 chunks to chroma.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "def generate_data_store():\n",
        "  \"\"\"\n",
        "  Function to generate vector database in chroma from documents.\n",
        "  \"\"\"\n",
        "  documents = load_documents() # Load documents from a source\n",
        "  chunks = split_text(documents) # Split documents into manageable chunks\n",
        "  client = save_to_chroma(chunks) # Save the processed data to a data store\n",
        "\n",
        "  return client\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "# Generate the data store\n",
        "client = generate_data_store()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eoItiKouy9wi"
      },
      "outputs": [],
      "source": [
        "query_text = \"List the CM interfaces\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nxMBS0HXy-1C"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        " - -\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TCHkttRqy_96"
      },
      "outputs": [],
      "source": [
        "def query_rag(query_text):\n",
        "  \"\"\"\n",
        "  Query a Retrieval-Augmented Generation (RAG) system using Chroma database and Germini.\n",
        "  Args:\n",
        "    - query_text (str): The text to query the RAG system with.\n",
        "  Returns:\n",
        "    - formatted_response (str): Formatted response including the generated text and sources.\n",
        "    - response_text (str): The generated response text.\n",
        "  \"\"\"\n",
        "  # YOU MUST - Use same embedding function as before\n",
        "\n",
        "  # Prepare the database\n",
        "  #db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
        "\n",
        "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings, collection_name=COLLECTION_NAME)\n",
        "\n",
        "  #docs = db.similarity_search(query_text)\n",
        "#  collection = db.get()\n",
        "\n",
        "#  print(\"collection\")\n",
        "#  print(collection)\n",
        "\n",
        "  #collection = db.get_or_create_collection(COLLECTION_NAME)\n",
        "\n",
        "  # Retrieving the context from the DB using similarity search\n",
        "  results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "\n",
        "  #print( f\"results:\")\n",
        "  #pprint(results)\n",
        "\n",
        "  #wrapped_results = textwrap.fill(str(results),width=80)\n",
        "  #print(wrapped_results)\n",
        "\n",
        "  # Check if there are any matching results or if the relevance score is too low\n",
        "  #print(f\"results:{len(results)}\")\n",
        "  if len(results) == 0:\n",
        "    print(f\"No matching results found.\")\n",
        "    return\n",
        "\n",
        "  # Extract similarity scores from results\n",
        "  similarity_scores = [score for _, score in results]\n",
        "\n",
        "  print(f\"similarity_scores{similarity_scores}\")\n",
        "\n",
        "  # Find the lowest similarity score\n",
        "  highest_similarity = max(similarity_scores)\n",
        "\n",
        "  if highest_similarity < 0.7:\n",
        "    print(f\"Warning: Only poor matches to query have been found in vector store. Highest similirity is {highest_similarity: .0%}\")\n",
        "\n",
        "  # Combine context from matching documents\n",
        "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "\n",
        "  # Create prompt template using context and query text\n",
        "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "\n",
        "  # Initialize OpenAI chat model\n",
        "  model = llm\n",
        "\n",
        "  #print(prompt)\n",
        "\n",
        "  # Generate response text based on the prompt\n",
        "  response_text = model.invoke(prompt)\n",
        "\n",
        "   # Get sources of the matching documents\n",
        "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "\n",
        "  #print(f\"sources:{sources}\")\n",
        "\n",
        "  # Format and return response including generated text and sources\n",
        "  formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "  return formatted_response, response_text\n",
        "\n",
        "# Let's call our function we have defined\n",
        "#formatted_response, response_text = query_rag(\"What does the term CMCI mean?\")\n",
        "# and finally, inspect our final response!\n",
        "#print(response_text.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let the user query the documents:\n",
        "while (True):\n",
        "  user_query = input(\"Type in your question: \")\n",
        "  if (user_query == \"exit\"):\n",
        "    break\n",
        "  formatted_response, response_text = query_rag(user_query)\n",
        "  #print(formatted_response)\n",
        "  print(response_text.content)\n",
        "  print(\"---------------\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "WIcJ36YOdFwE",
        "outputId": "d8c246f0-c6bc-46fa-a735-a18830204f58"
      },
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type in your question: What does CM mean?\n",
            "similarity_scores[0.6206714934656465, 0.5110424018398245, 0.49300320313640067]\n",
            "Warning: Only poor matches to query have been found in vector store. Highest similirity is  62%\n",
            "Based on the provided text, CM most likely stands for **Cable Modem**. \n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **\"IF3-MIB\"** refers to an SNMP Management Information Base (MIB) related to cable modem interfaces.\n",
            "* **\"docsIfCmStatusValue\"** and **\"docsIfCmtsCmStatusValue\"** suggest objects related to the status of a CM and CMTS (Cable Modem Termination System).\n",
            "* The reference to **\"Data-Over-Cable Service Interface Specifications\"** further strengthens the connection to cable modem technology. \n",
            "\n",
            "Therefore, considering the context, CM most likely refers to Cable Modem. \n",
            "\n",
            "---------------\n",
            "Type in your question: What does CMCI mean?\n",
            "similarity_scores[0.5544772547746797, 0.5454541917084196, 0.530824780565154]\n",
            "Warning: Only poor matches to query have been found in vector store. Highest similirity is  55%\n",
            "Based on the provided text, CMCI stands for **Customer-premises equipment Metropolitan Area Network Circuit Interface**. \n",
            "\n",
            "The text states: \"The term 'CMCI port' describes physical interfaces to which externally connected CPE devices can attach.\" \n",
            "\n",
            "Since CPE stands for Customer-premises equipment, we can infer that CMCI is a type of interface on the network side that connects to CPE devices. \n",
            "\n",
            "---------------\n",
            "Type in your question: What message must the CM use during registration?\n",
            "similarity_scores[0.6635488311549314, 0.6567347394219325, 0.644667717371627]\n",
            "Warning: Only poor matches to query have been found in vector store. Highest similirity is  66%\n",
            "**Registration Request (REG-REQ) messages** \n",
            "\n",
            "The text states that \"A DOCSIS 2.0+IPv6 CM MUST perform registration [...] as defined in [RFIv2.0]\". It then refers to \"Registration Request messages transmitted to the CMTS\". \n",
            "\n",
            "---------------\n",
            "Type in your question: Exit\n",
            "similarity_scores[0.5095029832703325, 0.4662117559830107, 0.4648414132663119]\n",
            "Warning: Only poor matches to query have been found in vector store. Highest similirity is  51%\n",
            "I cannot answer your question. There is no question in the provided context. \n",
            "\n",
            "---------------\n",
            "Type in your question: exit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zfRk6_ijIpO_sMpJHtCidmfCq05M0o_Y",
      "authorship_tag": "ABX9TyPVCxylvPJzUNLOuUwNSaHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}